{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23d7cfc9-dfeb-4328-b4db-3277459ca935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Loaded and Split ---\n",
      "X_train shape: (105, 4)\n",
      "X_test shape: (45, 4)\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Load Data and Split for Both Questions\n",
    "# -------------------------------------------------------------------\n",
    "# Load the Iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"--- Data Loaded and Split ---\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(\"-\" * 30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc6506-01d2-4888-81a2-49909a388e9f",
   "metadata": {},
   "source": [
    "# **QUES 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7969194f-39d5-4a24-9839-e683f42db8d9",
   "metadata": {},
   "source": [
    "**(Gaussian Naïve Bayes Classifier)** \n",
    "<br>\n",
    "**Implement Gaussian Naïve Bayes**\n",
    "<br>\n",
    "**Classifier on the Iris dataset from sklearn.datasets using\n",
    "i. Step-by-step implementation\n",
    "ii. In-built function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2fb2992-f321-4220-8a93-44e56643c392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- (i) Step-by-Step Implementation ---\n",
      "Custom GNB Accuracy: 0.9778\n",
      "Classification Report (Custom GNB):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.97        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "--- (ii) In-built scikit-learn Implementation ---\n",
      "scikit-learn GNB Accuracy: 0.9778\n",
      "Classification Report (scikit-learn GNB):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.97        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class CustomGaussianNB:\n",
    "    \"\"\"\n",
    "    Custom implementation of Gaussian Naïve Bayes classifier.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "\n",
    "        # Initialize arrays for mean, variance, and priors\n",
    "        # Shape: (n_classes, n_features)\n",
    "        self.mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self.var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        # Shape: (n_classes,)\n",
    "        self.priors = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        # Add a small value (epsilon) for numerical stability\n",
    "        self.epsilon = 1e-9\n",
    "\n",
    "        # Calculate mean, variance, and prior for each class\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            # Get all samples belonging to class c\n",
    "            X_c = X[y == c]\n",
    "            \n",
    "            # Calculate mean and variance (feature-wise)\n",
    "            self.mean[idx, :] = X_c.mean(axis=0)\n",
    "            self.var[idx, :] = X_c.var(axis=0) + self.epsilon\n",
    "            \n",
    "            # Calculate prior probability of class c\n",
    "            self.priors[idx] = X_c.shape[0] / float(n_samples)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict the class for each sample in X\n",
    "        y_pred = [self._predict_sample(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict_sample(self, x):\n",
    "        # Calculate posterior probability for each class\n",
    "        posteriors = []\n",
    "\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            # Calculate log prior\n",
    "            log_prior = np.log(self.priors[idx])\n",
    "            \n",
    "            # Calculate log-likelihood using the Gaussian PDF formula\n",
    "            # log(P(x_i | c)) = -0.5 * log(2*pi*var_i) - 0.5 * ((x_i - mean_i)^2 / var_i)\n",
    "            log_likelihood_features = -0.5 * np.log(2 * np.pi * self.var[idx, :]) - \\\n",
    "                                      0.5 * ((x - self.mean[idx, :])**2 / self.var[idx, :])\n",
    "            \n",
    "            # Sum log-likelihoods over all features\n",
    "            total_log_likelihood = np.sum(log_likelihood_features)\n",
    "            \n",
    "            # Calculate posterior (sum of log prior and log likelihood)\n",
    "            posterior = log_prior + total_log_likelihood\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        # Return the class with the highest posterior probability\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "print(\"--- (i) Step-by-Step Implementation ---\")\n",
    "custom_gnb = CustomGaussianNB()\n",
    "custom_gnb.fit(X_train, y_train)\n",
    "y_pred_custom = custom_gnb.predict(X_test)\n",
    "custom_accuracy = accuracy_score(y_test, y_pred_custom)\n",
    "\n",
    "print(f\"Custom GNB Accuracy: {custom_accuracy:.4f}\")\n",
    "print(\"Classification Report (Custom GNB):\")\n",
    "print(classification_report(y_test, y_pred_custom))\n",
    "\n",
    "\n",
    "# --- (ii) In-built function implementation  ---\n",
    "\n",
    "print(\"--- (ii) In-built scikit-learn Implementation ---\")\n",
    "sklearn_gnb = GaussianNB()\n",
    "sklearn_gnb.fit(X_train, y_train)\n",
    "y_pred_sklearn = sklearn_gnb.predict(X_test)\n",
    "sklearn_accuracy = accuracy_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(f\"scikit-learn GNB Accuracy: {sklearn_accuracy:.4f}\")\n",
    "print(\"Classification Report (scikit-learn GNB):\")\n",
    "print(classification_report(y_test, y_pred_sklearn))\n",
    "print(\"-\" * 30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d623926-f1b5-4c90-9e5b-688f5f69d42a",
   "metadata": {},
   "source": [
    "# **QUES 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a8af6-e710-4e84-a671-e03f7ed68807",
   "metadata": {},
   "source": [
    "**Explore about GridSearchCV toot in scikit-learn. This is a tool that is\n",
    "often used for tuning hyperparameters of machine learning models. Use\n",
    "this tool to find the best value of K for K-NN Classifier using any dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ccc9628-7880-4bc8-b024-ba6edde20207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "GridSearchCV fitting complete.\n",
      "Best K value (n_neighbors): 1\n",
      "Best cross-validation accuracy: 0.9524\n",
      "\n",
      "Accuracy of the best K-NN model on the TEST set: 1.0000\n",
      "Classification Report (Best K-NN):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# 2. Define the parameter grid to search\n",
    "# We'll search for the best K from 1 to 30\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 31))\n",
    "}\n",
    "\n",
    "# 3. Instantiate GridSearchCV\n",
    "# cv=5 means 5-fold cross-validation\n",
    "# verbose=1 shows progress\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1  # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# 4. Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 5. Print the best parameters and the best score\n",
    "print(\"\\nGridSearchCV fitting complete.\")\n",
    "print(f\"Best K value (n_neighbors): {grid_search.best_params_['n_neighbors']}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# 6. Use the best model (best_estimator_) to make predictions on the test set\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "y_pred_knn = best_knn_model.predict(X_test)\n",
    "knn_test_accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"\\nAccuracy of the best K-NN model on the TEST set: {knn_test_accuracy:.4f}\")\n",
    "print(\"Classification Report (Best K-NN):\")\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "print(\"-\" * 30 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
